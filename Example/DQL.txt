Links:
https://github.com/shakedzy/notebooks/blob/master/q_learning_and_dqn/Q%20Learning%20and%20Deep%20Q%20Network.ipynb
https://towardsdatascience.com/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677
https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/


Deep Q learning:
Q(s,a) = Q(s,a) + alpha * [reward next action + gamma * max(Q(s+1,a)) - Q(s,a)]
Target is reward next action + gamma * max(Q(s+1,a))

Problem - constantly changing input and output. Not good for standard neural network.
Solution:
	1 - Use 2 networks, target network and prediction network. Same architectures.
		Target network has frozen parameters (weights). Every C iterations, copy weights from prediction network to target network.
			Leads to more stable training because target function is fixed for periods of time.
		Prediction network create predictions.
		
Flow: (note - θ represents the trainable weights of the network.)
	Preprocess and feed came into DQN. Returns Q values of all possible actions in that state.
	Select an action using policy. Example is argmax(Q(state, action, weight))
	Perform action in state and move to new state to get reward. This is the preprocessed image of the next game screen. Store this transition in replay buffer as <s,a,r,s'>
	Sample random batches of transitions from replay buffer and calculate loss.
	Loss for Q learning equation is (R + gamma * max(Q(s',a':theta')) - Q(s,a:phi))^2 (MSE)
		Squared difference between predicted Q and target Q.
	Perform gradient descent with actual neural network paramters for prediction net.
	Every C iterations, copy to target neural net.
	Repeat steps.
	
Network arch:
	Output is the actions for a given state.
	
Example:
tf.reset_default_graph()
tf.set_random_seed(seed)
qnn = QNetwork(hidden_layers_size=[20,20], gamma=gamma, learning_rate=0.001)
memory = ReplayMemory(memory_size)
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for g in range(num_of_games):
    game_over = False
    game.reset()
    total_reward = 0
    while not game_over:
        counter += 1
		#Get the current state. In mario, this would be a frame.
        state = np.copy(game.board)
		#Every so often just pick a random action
        if random.random() < epsilon:
            action = random.randint(0,3)
        else:
			#Feed the state through the QNN. Input size is 4 - observation space size. Fram would be X by Y.
            pred = np.squeeze(sess.run(qnn.output,feed_dict={qnn.states: np.expand_dims(game.board,axis=0)}))
			#Get the highest prediction from the network. Hidden layers have RELU, output has none.
            action = np.argmax(pred)
		#Feed action through game. Get reward.
        reward, game_over = game.play(action)
        total_reward += reward
		#Get the state as a result of 
        next_state = np.copy(game.board)
		#Append to experience replay memory.
        memory.append({'state':state,'action':action,'reward':reward,'next_state':next_state,'game_over':game_over})
		#Every n iterations of game, train network
        if counter % batch_size == 0:
            # Network training. Get random sample from the network.
            batch = memory.sample(batch_size)
			#Run the next state for each item in memory batch through the output.
            q_target = sess.run(qnn.output,feed_dict={qnn.states: np.array(list(map(lambda x: x['next_state'], batch)))})
			#Get if game over for each item in batch.
            terminals = np.array(list(map(lambda x: x['game_over'], batch)))
            for i in range(terminals.size):
				#If game over in any of the preds
                if terminals[i]:
                    # Remember we use the network's own predictions for the next state while calculatng loss.
                    # Terminal states have no Q-value, and so we manually set them to 0, as the network's predictions
                    # for these states is meaningless
					#Set the state to 0's for game over.
                    q_target[i] = np.zeros(game.board_size)
			"""
			cost:
				Feed dict:
					states : states from batch
					reward : reward from batch
					enum_actions : Action choices. Choice A to B, B to C, etc.
					q_target : the target values.
				
			"""
            _, cost = sess.run([qnn.optimizer, qnn.cost], 
                               feed_dict={qnn.states: np.array(list(map(lambda x: x['state'], batch))),
                               qnn.r: np.array(list(map(lambda x: x['reward'], batch))),
                               qnn.enum_actions: np.array(list(enumerate(map(lambda x: x['action'], batch)))),
                               qnn.q_target: q_target})
            c_list.append(cost)
    r_list.append(total_reward)
print('Final cost: {}'.format(c_list[-1]))