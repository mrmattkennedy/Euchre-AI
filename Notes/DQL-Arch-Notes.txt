class:
	Flow is: Create placeholder tensor with self.states, called layer.
        Then, reassign layer to a new dense layer for every hidden layer.
        Last, create output as a dense layer referencing layer, which references every prior layer.
        So output depends on states, which is why feed dict with states is required.


	q_target (output)
		placeholder: shape is (None, output_size), float32
	r (reward)
		placeholder: no shape (not an array, just a value), float32
	states ?
	

Flow of network training:
Get a batch (random sample from memory)
q_target = sess.run(qnn.output,feed_dict={qnn.states: np.array(list(map(lambda x: x['next_state'], batch)))})
	assign qnn.output, feed in the previous game states from batch as states
Get any terminal (game over) in the batch
Loop through terminals, if any are True, set q_target to all 0's
	Terminal states have no Q-value because we use network's own predictions for the next state

Last:
_, cost = sess.run([qnn.optimizer, qnn.cost], 
                               feed_dict={qnn.states: np.array(list(map(lambda x: x['state'], batch))),
                               qnn.r: np.array(list(map(lambda x: x['reward'], batch))),
                               qnn.enum_actions: np.array(list(enumerate(map(lambda x: x['action'], batch)))),
                               qnn.q_target: q_target})
	run optimizer and cost
	feed dict:
		states, given values of current states from each item in batch
		rewards, from each item in batch
		enum actions, enumerate batch actions
		q_target, target for each item in batch
	optimizer minimizes cost
		cost is MSE of labels and preds
		self.labels = self.r + gamma * tf.reduce_max(input_tensor=self.q_target, axis=1)
		self.predictions = tf.gather_nd(self.output,indices=self.enum_actions)

		q target is all 4 possible actions in the following state after some action
		predictions are the outputs - feed in the states, get the outputs of each state


	batch will feed each state in to get outputs
	q_target is when next_state is fed into network for each item in batch (output is possible actions)
	labels are rewards * gamma * max possible action for next state
	predictions are original states fed in with enumerated action choices
		we had an original state, fed it through and got some reward. label is the best action of the followup state?
	
	Cost = [Q(s,a;theta) - (r(s,a) + gamma*maxQ(s',a;phi)]^2
	first part is the actions after being fed a state. hence, why we enumerate the actions to see what we picked
		Q value of original state
	second part is reward + gamma * max Q value after being fed followup state for the action we chose
		feed in new state, get Q values for each action, pick max

x = tf.placeholder(tf.float32)
y = x * 42
sess.run(y, feed_dict={x: 2})
	y is dependent on x, so get value of y, and need some value fed to x through feed dict

self.output = tf.compat.v1.layers.dense(inputs=layer, units=output_size,
	kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode="fan_avg", distribution="uniform", seed=seed))

pred = np.squeeze(sess.run(qnn.output,feed_dict={qnn.states: np.expand_dims(game.board,axis=0)}))
	qnn.output dependent on qnn.states (see class flow description)
	qnn.states assigned the value of the game board
	expand dims - change from shape (4,) to (1,4) so 2 dim
	sqeeze removes single dimensions (shape of (1,3,1) goes to just (3, 1)



idea is:
Preprocess and feed the game screen (state s) to our DQN, which will return the Q-values of all possible actions in the state
Select an action using the epsilon-greedy policy. With the probability epsilon, we select a random action a and with probability 1-epsilon, we select an action that has a maximum Q-value, such as a = argmax(Q(s,a,w))
Perform this action in a state s and move to a new state s’ to receive a reward. This state s’ is the preprocessed image of the next game screen. We store this transition in our replay buffer as <s,a,r,s’>
Next, sample some random batches of transitions from the replay buffer and calculate the loss
It is known that: which is just the squared difference between target Q and predicted Q
Perform gradient descent with respect to our actual network parameters in order to minimize this loss
After every C iterations, copy our actual network weights to the target network weights
Repeat these steps for M number of episodes




Converting to TF 2.0:
https://stackoverflow.com/questions/58986126/replacing-placeholder-for-tensorflow-v2

	flow starts with sess run to the output - run it through the network
		need to replace placeholder network with regular network
		convert game states to tensors
	q_target (output)
		placeholder: shape is (None, output_size), float32
	r (reward)
		placeholder: no shape (not an array, just a value), float32
	states ?